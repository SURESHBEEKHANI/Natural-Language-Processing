{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNktTZQfQMY/5t80aRRdqrR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Natural-Language-Processing/blob/main/Text_Preprocessing_TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF: Term Frequency-Inverse Document Frequency\n",
        "\n",
        "\n",
        "\n",
        "Understanding TF-IDF: A Quick Overview\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency) is a crucial technique in text analysis and natural language processing. It evaluates the significance of a word in a document relative to a collection of documents (corpus).\n",
        "\n",
        "Key Components\n",
        "Term Frequency (TF): Measures how often a term appears in a document, normalized to prevent bias towards longer documents.\n",
        "\n",
        "# Applications\n",
        "\n",
        "\n",
        "Information Retrieval: Ranks documents by relevance to a query.\n",
        "Text Mining: Extracts significant words from documents.\n",
        "Document Clustering: Measures document similarity for tasks like topic modeling.\n",
        "Feature Extraction: Converts text to numerical features for machine learning."
      ],
      "metadata": {
        "id": "w199sMjDiBzH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1fMpOwBMRux"
      },
      "outputs": [],
      "source": [
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopword')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Defining a paragraph of text about Artificial Intelligence (AI)\n",
        "# This paragraph will be used to demonstrate text preprocessing techniques\n",
        "paragraph = \"\"\"Artificial Intelligence (AI) is a transformative technology that mimics human intelligence\n",
        "to perform tasks such as learning, reasoning, problem-solving, and decision-making. It encompasses various subfields\n",
        "including machine learning, natural language processing, computer vision, and robotics. AI systems analyze\n",
        "vast amounts of data to identify patterns, make predictions, and improve their performance over time through iterative\n",
        "processes. This technology has vast applications across industries, from healthcare, where it aids in diagnosing\n",
        "diseases and personalizing treatment plans, to finance, where it enhances fraud detection and automates trading.\n",
        "AI also powers virtual assistants like Siri and Alexa, self-driving cars, and advanced manufacturing processes.\n",
        "As AI continues to evolve, it promises to revolutionize the way we live and work, offering unprecedented opportunities\n",
        "for innovation and efficiency while also posing ethical and societal challenges that must be carefully managed.\"\"\"\n",
        "\n",
        "#Text Cleaning  Process\n",
        "\n",
        "import re  # Allows using regular expressions for advanced text processing\n",
        "\n",
        "from nltk.corpus import stopwords  # Provides a set of common words like \"the\", \"is\", \"in\" to filter out\n",
        "from nltk.stem import WordNetLemmatizer  # Helps in reducing words to their base or dictionary form\n",
        "from nltk.stem import PorterStemmer  # Assists in reducing words to their root form\n",
        "\n"
      ],
      "metadata": {
        "id": "QmHrRnCjj1MM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create Object for stemming\n",
        "stemmer=PorterStemmer()\n",
        "#Create objects  for lemmatization\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "#Use Sent_tokenize for convert Crops or Paragraph into senencent\n",
        "sentences=nltk.sent_tokenize(paragraph)\n",
        "\n",
        "\n",
        "corpus=[]"
      ],
      "metadata": {
        "id": "XC4gGLlPkCdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download NLTK stopwords data\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "xAmJFlTVkOlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterating over each sentence in the 'sentences' list\n",
        "for i in range(len(sentences)):\n",
        "    # Removing non-alphabetical characters and replacing them with spaces\n",
        "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
        "\n",
        "    # Converting all characters to lowercase\n",
        "    review = review.lower()\n",
        "\n",
        "    # Splitting the sentence into individual words\n",
        "    review = review.split()\n",
        "\n",
        "    # Lemmatizing each word (reducing them to their base form) if it's not a stopword\n",
        "    review = [lemmatizer.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "\n",
        "    # Joining the lemmatized words back into a sentence\n",
        "    review = ' '.join(review)\n",
        "\n",
        "    # Adding the processed sentence to the 'corpus' list\n",
        "    corpus.append(review)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "uRaJqQd1kTlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "id": "ADxIZ7iInTo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary tools from the sklearn library\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "# Creating a tool to calculate TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Using the tool to calculate TF-IDF values for our sample documents\n",
        "# This will analyze the importance of each word in the context of all documents\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XE0E6muBkx75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tfidf_matrix)"
      ],
      "metadata": {
        "id": "7lXIhMmpnGz2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}