{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM2vQaEAhQUELTsnrDGQ9Oe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Natural-Language-Processing/blob/main/Stemming_And_Lemmatization_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Stemming and Lemmatization\n",
        "Stemming and lemmatization are both techniques used in natural language processing to reduce words to their base forms, but they differ in their approaches and accuracy.\n",
        "\n",
        "## Stemming:\n",
        "\n",
        "Reduces words to their base forms using simple rules.\n",
        "May produce non-words (e.g., \"happily\" -> \"happi\").\n",
        "Fast and suitable for speed-critical applications like real-time search engines.\n",
        "\n",
        "## Lemmatization:\n",
        "\n",
        "Reduces words to their dictionary forms considering context and part of speech.\n",
        "Produces valid words (e.g., \"better\" -> \"good\").\n",
        "More accurate but slower, suitable for accuracy-critical applications like machine translation and text classification.\n",
        "\n",
        "#Key Difference\n",
        "\n",
        "Stemming: Fast, less accurate, produces non-words.\n",
        "\n",
        "Lemmatization: Slower, more accurate, produces valid words.\n"
      ],
      "metadata": {
        "id": "K0qC8F0KHEGO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjOI8jmEGNhV"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # This downloads the necessary resources for tokenization\n",
        "nltk.download('averaged_perceptron_tagger')  # For part-of-speech tagging\n",
        "nltk.download('wordnet')  # For lemmatization\n",
        "nltk.download('stopwords')  # For stopwords\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PorterStemmer:  \n",
        "PorterStemmer is a widely-used stemming algorithm that reduces words to their base or root form. It follows a set of rules and heuristics to strip affixes from words, aiming to produce the most common stem for related words. It's computationally efficient but may not always produce the most linguistically accurate stems."
      ],
      "metadata": {
        "id": "i2EzDOayhzX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the Natural Language Toolkit (nltk) library for text preprocessing\n",
        "import nltk\n",
        "\n",
        "# Importing the PorterStemmer class from the nltk.stem module\n",
        "# PorterStemmer is used to reduce words to their root form, which helps in text normalization\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Importing the stopwords from the nltk.corpus module\n",
        "# Stopwords are common words like \"the\", \"is\", \"in\" that are usually removed in text processing\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Downloading the 'punkt' and 'stopwords' packages from nltk\n",
        "# 'punkt' is a pre-trained model for tokenizing text into sentences and words\n",
        "# 'stopwords' contains a list of common stopwords for various languages\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Defining a paragraph of text about Artificial Intelligence (AI)\n",
        "# This paragraph will be used to demonstrate text preprocessing techniques\n",
        "paragraph = \"\"\"Artificial Intelligence (AI) is a transformative technology that mimics human intelligence\n",
        "to perform tasks such as learning, reasoning, problem-solving, and decision-making. It encompasses various subfields\n",
        "including machine learning, natural language processing, computer vision, and robotics. AI systems analyze\n",
        "vast amounts of data to identify patterns, make predictions, and improve their performance over time through iterative\n",
        "processes. This technology has vast applications across industries, from healthcare, where it aids in diagnosing\n",
        "diseases and personalizing treatment plans, to finance, where it enhances fraud detection and automates trading.\n",
        "AI also powers virtual assistants like Siri and Alexa, self-driving cars, and advanced manufacturing processes.\n",
        "As AI continues to evolve, it promises to revolutionize the way we live and work, offering unprecedented opportunities\n",
        "for innovation and efficiency while also posing ethical and societal challenges that must be carefully managed.\"\"\"\n",
        "\n",
        "# Tokenizing the paragraph into sentences\n",
        "# This breaks down the paragraph into individual sentences for further processing\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "\n",
        "# Creating an instance of the PorterStemmer\n",
        "# This instance will be used to stem words, reducing them to their base form\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Getting the list of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Iterating over each sentence in the paragraph\n",
        "for i in range(len(sentences)):\n",
        "    # Tokenizing each sentence into words\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "\n",
        "    # Removing stopwords and stemming the remaining words\n",
        "    stemmed_words = [stemmer.stem(word) for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # Joining the stemmed words back into a sentence\n",
        "    sentences[i] = ' '.join(stemmed_words)\n",
        "\n",
        "# Printing the processed sentences\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n"
      ],
      "metadata": {
        "id": "7Q06aJzWI1nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LancasterStemmer:\n",
        "\n",
        "LancasterStemmer is another stemming algorithm that, like PorterStemmer, reduces words to their base form. However, it tends to be more aggressive in its stemming process, which can sometimes lead to stems that are less intuitive or natural compared to PorterStemmer. It's known for its fast execution and aggressive stemming rules."
      ],
      "metadata": {
        "id": "yARZsa3Fh9Tc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the Natural Language Toolkit (nltk) library for text preprocessing\n",
        "import nltk\n",
        "\n",
        "# Importing the LancasterStemmer class from the nltk.stem module\n",
        "# LancasterStemmer is used to reduce words to their root form, which helps in text normalization\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "# Importing the stopwords from the nltk.corpus module\n",
        "# Stopwords are common words like \"the\", \"is\", \"in\" that are usually removed in text processing\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Downloading the 'punkt' and 'stopwords' packages from nltk\n",
        "# 'punkt' is a pre-trained model for tokenizing text into sentences and words\n",
        "# 'stopwords' contains a list of common stopwords for various languages\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Defining a paragraph of text about Artificial Intelligence (AI)\n",
        "# This paragraph will be used to demonstrate text preprocessing techniques\n",
        "paragraph = \"\"\"Artificial Intelligence (AI) is a transformative technology that mimics human intelligence\n",
        "to perform tasks such as learning, reasoning, problem-solving, and decision-making. It encompasses various subfields\n",
        "including machine learning, natural language processing, computer vision, and robotics. AI systems analyze\n",
        "vast amounts of data to identify patterns, make predictions, and improve their performance over time through iterative\n",
        "processes. This technology has vast applications across industries, from healthcare, where it aids in diagnosing\n",
        "diseases and personalizing treatment plans, to finance, where it enhances fraud detection and automates trading.\n",
        "AI also powers virtual assistants like Siri and Alexa, self-driving cars, and advanced manufacturing processes.\n",
        "As AI continues to evolve, it promises to revolutionize the way we live and work, offering unprecedented opportunities\n",
        "for innovation and efficiency while also posing ethical and societal challenges that must be carefully managed.\"\"\"\n",
        "\n",
        "# Tokenizing the paragraph into sentences\n",
        "# This breaks down the paragraph into individual sentences for further processing\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "\n",
        "# Creating an instance of the LancasterStemmer\n",
        "# This instance will be used to stem words, reducing them to their base form\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "# Getting the list of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Iterating over each sentence in the paragraph\n",
        "for i in range(len(sentences)):\n",
        "    # Tokenizing each sentence into words\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "\n",
        "    # Removing stopwords and stemming the remaining words\n",
        "    stemmed_words = [stemmer.stem(word) for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # Joining the stemmed words back into a sentence\n",
        "    sentences[i] = ' '.join(stemmed_words)\n",
        "\n",
        "# Printing the processed sentences\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n"
      ],
      "metadata": {
        "id": "XwNvDH8jJdSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RegexpStemmer:\n",
        "\n",
        "RegexpStemmer is a stemming algorithm provided by NLTK that allows customization using regular expressions. It enables specific rules for stemming, making it suitable for tasks where tailored patterns and transformations are required."
      ],
      "metadata": {
        "id": "Vq_42gHXiebL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the Natural Language Toolkit (nltk) library for text preprocessing\n",
        "import nltk\n",
        "\n",
        "# Importing the RegexpStemmer class from the nltk.stem module\n",
        "# RegexpStemmer is used to reduce words to their root form based on regular expressions\n",
        "from nltk.stem import RegexpStemmer\n",
        "\n",
        "# Importing the stopwords from the nltk.corpus module\n",
        "# Stopwords are common words like \"the\", \"is\", \"in\" that are usually removed in text processing\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Downloading the 'punkt' and 'stopwords' packages from nltk\n",
        "# 'punkt' is a pre-trained model for tokenizing text into sentences and words\n",
        "# 'stopwords' contains a list of common stopwords for various languages\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Defining a paragraph of text about Artificial Intelligence (AI)\n",
        "# This paragraph will be used to demonstrate text preprocessing techniques\n",
        "paragraph = \"\"\"Artificial Intelligence (AI) is a transformative technology that mimics human intelligence\n",
        "to perform tasks such as learning, reasoning, problem-solving, and decision-making. It encompasses various subfields\n",
        "including machine learning, natural language processing, computer vision, and robotics. AI systems analyze\n",
        "vast amounts of data to identify patterns, make predictions, and improve their performance over time through iterative\n",
        "processes. This technology has vast applications across industries, from healthcare, where it aids in diagnosing\n",
        "diseases and personalizing treatment plans, to finance, where it enhances fraud detection and automates trading.\n",
        "AI also powers virtual assistants like Siri and Alexa, self-driving cars, and advanced manufacturing processes.\n",
        "As AI continues to evolve, it promises to revolutionize the way we live and work, offering unprecedented opportunities\n",
        "for innovation and efficiency while also posing ethical and societal challenges that must be carefully managed.\"\"\"\n",
        "\n",
        "# Tokenizing the paragraph into sentences\n",
        "# This breaks down the paragraph into individual sentences for further processing\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "\n",
        "# Creating an instance of the RegexpStemmer\n",
        "# This instance will be used to stem words, reducing them to their base form based on a regular expression\n",
        "# The regular expression removes common suffixes\n",
        "stemmer = RegexpStemmer('ing$|s$|ed$|er$|ly$', min=4)\n",
        "\n",
        "# Getting the list of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Iterating over each sentence in the paragraph\n",
        "for i in range(len(sentences)):\n",
        "    # Tokenizing each sentence into words\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "\n",
        "    # Removing stopwords and stemming the remaining words\n",
        "    stemmed_words = [stemmer.stem(word) for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # Joining the stemmed words back into a sentence\n",
        "    sentences[i] = ' '.join(stemmed_words)\n",
        "\n",
        "# Printing the processed sentences\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n"
      ],
      "metadata": {
        "id": "tcnFg3LDKKOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SnowballStemmer:\n",
        "SnowballStemmer, also known as Porter2 or Martin Porter's stemmer, is an extension and improvement upon the original PorterStemmer algorithm. It supports stemming in multiple languages and provides more accurate stems for many words compared to PorterStemmer. It's designed to be more efficient and linguistically precise."
      ],
      "metadata": {
        "id": "GA9k3hhWiE8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the Natural Language Toolkit (nltk) library for text preprocessing\n",
        "import nltk\n",
        "\n",
        "# Importing the SnowballStemmer class from the nltk.stem module\n",
        "# SnowballStemmer is used to reduce words to their root form\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Importing the stopwords from the nltk.corpus module\n",
        "# Stopwords are common words like \"the\", \"is\", \"in\" that are usually removed in text processing\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Downloading the 'punkt' and 'stopwords' packages from nltk\n",
        "# 'punkt' is a pre-trained model for tokenizing text into sentences and words\n",
        "# 'stopwords' contains a list of common stopwords for various languages\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Defining a paragraph of text about Artificial Intelligence (AI)\n",
        "# This paragraph will be used to demonstrate text preprocessing techniques\n",
        "paragraph = \"\"\"Artificial Intelligence (AI) is a transformative technology that mimics human intelligence\n",
        "to perform tasks such as learning, reasoning, problem-solving, and decision-making. It encompasses various subfields\n",
        "including machine learning, natural language processing, computer vision, and robotics. AI systems analyze\n",
        "vast amounts of data to identify patterns, make predictions, and improve their performance over time through iterative\n",
        "processes. This technology has vast applications across industries, from healthcare, where it aids in diagnosing\n",
        "diseases and personalizing treatment plans, to finance, where it enhances fraud detection and automates trading.\n",
        "AI also powers virtual assistants like Siri and Alexa, self-driving cars, and advanced manufacturing processes.\n",
        "As AI continues to evolve, it promises to revolutionize the way we live and work, offering unprecedented opportunities\n",
        "for innovation and efficiency while also posing ethical and societal challenges that must be carefully managed.\"\"\"\n",
        "\n",
        "# Tokenizing the paragraph into sentences\n",
        "# This breaks down the paragraph into individual sentences for further processing\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "\n",
        "# Creating an instance of the SnowballStemmer for the English language\n",
        "# This instance will be used to stem words, reducing them to their base form\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "# Getting the list of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Iterating over each sentence in the paragraph\n",
        "for i in range(len(sentences)):\n",
        "    # Tokenizing each sentence into words\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "\n",
        "    # Removing stopwords and stemming the remaining words\n",
        "    stemmed_words = [stemmer.stem(word) for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # Joining the stemmed words back into a sentence\n",
        "    sentences[i] = ' '.join(stemmed_words)\n",
        "\n",
        "# Printing the processed sentences\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n"
      ],
      "metadata": {
        "id": "mlXzGFaOKI76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')  # Optional: for tokenizing sentences\n"
      ],
      "metadata": {
        "id": "WVNj4a-zPkJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WordNetLemmatizer\n",
        "\n",
        "WordNetLemmatizer is a tool provided by NLTK for lemmatization, which is the process of reducing words to their base or dictionary form (lemma). Unlike stemming, lemmatization considers the context and meaning of words to ensure accuracy."
      ],
      "metadata": {
        "id": "S0rs2NK0oc4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Defining a paragraph of text about Artificial Intelligence (AI)\n",
        "paragraph = \"\"\"Artificial Intelligence (AI) is a transformative technology that mimics human intelligence\n",
        "to perform tasks such as learning, reasoning, problem-solving, and decision-making. It encompasses various subfields\n",
        "including machine learning, natural language processing, computer vision, and robotics. AI systems analyze\n",
        "vast amounts of data to identify patterns, make predictions, and improve their performance over time through iterative\n",
        "processes. This technology has vast applications across industries, from healthcare, where it aids in diagnosing\n",
        "diseases and personalizing treatment plans, to finance, where it enhances fraud detection and automates trading.\n",
        "AI also powers virtual assistants like Siri and Alexa, self-driving cars, and advanced manufacturing processes.\n",
        "As AI continues to evolve, it promises to revolutionize the way we live and work, offering unprecedented opportunities\n",
        "for innovation and efficiency while also posing ethical and societal challenges that must be carefully managed.\"\"\"\n",
        "\n",
        "# Tokenizing the paragraph into sentences\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "\n",
        "# Initializing the WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Getting the list of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Iterating over each sentence in the paragraph\n",
        "for i in range(len(sentences)):\n",
        "    # Tokenizing each sentence into words\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "\n",
        "    # Lemmatizing each word and removing stopwords\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # Joining the lemmatized words back into a sentence\n",
        "    sentences[i] = ' '.join(lemmatized_words)\n",
        "\n",
        "# Printing the processed sentences\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n"
      ],
      "metadata": {
        "id": "uLMBJuqzQOci"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}