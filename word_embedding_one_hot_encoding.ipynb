{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN3gpGQ5qfJ+RzXb3bGe8jy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Natural-Language-Processing/blob/main/word_embedding_one_hot_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the one_hot function from the one_hot module of keras.preprocessing.text\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "\n",
        "# Usage of one_hot:\n",
        "# This function converts text into a list of integers, where each integer represents\n",
        "# the index of a word in a predefined dictionary.\n",
        "# It takes two main arguments: the text to encode and the size of the vocabulary.\n",
        "# Returns a list of integers (encoded representation of the text).\n",
        "\n",
        "# Example usage:\n",
        "# text = \"Hello world\"\n",
        "# vocab_size = 50\n",
        "# encoded_text = one_hot(text, vocab_size)\n",
        "# print(encoded_text)\n"
      ],
      "metadata": {
        "id": "8oHX2fXhv6wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of sentences to be encoded\n",
        "sentences = [\n",
        "    \"Deep learning models are powerful tools\",\n",
        "    \"Deep Artificial intelligence is transforming industries\",\n",
        "    \"Data science involves statistics and programming\",\n",
        "    \"Machine learning enables predictive analytics\",\n",
        "    \"Neural networks are inspired by the human brain\",\n",
        "    \"Big data is essential for modern analytics\",\n",
        "    \"Natural language processing is a key AI component\"\n",
        "]"
      ],
      "metadata": {
        "id": "t4pzzlCuwWoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the size of the vocabulary (vocab_size)\n",
        "# Vocab size is the total number of unique words the model can handle.\n",
        "vocab_size = 10000"
      ],
      "metadata": {
        "id": "EreNelaFxWno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **One_hot_encoding**"
      ],
      "metadata": {
        "id": "Jd_5mHQWxsmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the one_hot function to create a one-hot representation of each sentence\n",
        "onehot_rep = [one_hot(sentence, vocab_size) for sentence in sentences]\n",
        "\n",
        "# Print the one-hot representations\n",
        "print(onehot_rep)"
      ],
      "metadata": {
        "id": "82s2IGNZyyjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embedding RepresenQtation"
      ],
      "metadata": {
        "id": "S08O7Z4czkmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import  the Word embeddings layers from  tensorflow.keras\n",
        "from tensorflow.keras.layers import Embedding\n",
        "#Import  the Sequential liyers Create By Sequential Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "#import pad_sequences from tensorflow.keras.preprocessing.sequences\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "#import numpy for manipulate  numerical value\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "-LzWeaDgzkQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sentence length for input to the embedding model\n",
        "# This is the maximum length of each sentence; shorter sentences will be padded\n",
        "sent_length = 8\n",
        "\n",
        "# Use pad_sequences to ensure each sentence has the same length\n",
        "# 'pre' padding means that zeros will be added to the beginning of sentences that are shorter than sent_length\n",
        "embedded_docs = pad_sequences(onehot_rep, padding='pre', maxlen=sent_length)\n",
        "\n",
        "# Print the padded one-hot encoded sentences\n",
        "print(\"Padded one-hot encoded sentences:\", embedded_docs)"
      ],
      "metadata": {
        "id": "c3bRhbWl41dG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dim =10 #10 Features  Repentance\n",
        "\n",
        "model = Sequential() #add the Sequential Layer create Model In sequence-to-sequence\n",
        "\n",
        "#add Embedding and parameters\n",
        "model.add(Embedding(vocab_size, dim, input_length=sent_length))\n",
        "#add compliler compile to the model\n",
        "model.compile('adam', 'mse')"
      ],
      "metadata": {
        "id": "bdb3aYRd41N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "hEWAxzfy7_m4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}