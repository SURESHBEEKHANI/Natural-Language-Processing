{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNEWHqGQIyQAPevaxi1Ml7Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Natural-Language-Processing/blob/main/Bag_Of_Words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopword')"
      ],
      "metadata": {
        "id": "71m56zcyAAux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sp1FMyHS3B63"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "# Defining a paragraph of text about Artificial Intelligence (AI)\n",
        "# This paragraph will be used to demonstrate text preprocessing techniques\n",
        "paragraph = \"\"\"Artificial Intelligence (AI) is a transformative technology that mimics human intelligence\n",
        "to perform tasks such as learning, reasoning, problem-solving, and decision-making. It encompasses various subfields\n",
        "including machine learning, natural language processing, computer vision, and robotics. AI systems analyze\n",
        "vast amounts of data to identify patterns, make predictions, and improve their performance over time through iterative\n",
        "processes. This technology has vast applications across industries, from healthcare, where it aids in diagnosing\n",
        "diseases and personalizing treatment plans, to finance, where it enhances fraud detection and automates trading.\n",
        "AI also powers virtual assistants like Siri and Alexa, self-driving cars, and advanced manufacturing processes.\n",
        "As AI continues to evolve, it promises to revolutionize the way we live and work, offering unprecedented opportunities\n",
        "for innovation and efficiency while also posing ethical and societal challenges that must be carefully managed.\"\"\"\n",
        "\n",
        "#Text Cleaning  Process\n",
        "\n",
        "import re  # Allows using regular expressions for advanced text processing\n",
        "\n",
        "from nltk.corpus import stopwords  # Provides a set of common words like \"the\", \"is\", \"in\" to filter out\n",
        "from nltk.stem import WordNetLemmatizer  # Helps in reducing words to their base or dictionary form\n",
        "from nltk.stem import PorterStemmer  # Assists in reducing words to their root form\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create Object for stemming\n",
        "stemmer=PorterStemmer()\n",
        "#Create objects  for lemmatization\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "#Use Sent_tokenize for convert Crops or Paragraph into senencent\n",
        "sentences=nltk.sent_tokenize(paragraph)\n",
        "\n",
        "\n",
        "corpus=[]"
      ],
      "metadata": {
        "id": "RJvQ9_8X980B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download NLTK stopwords data\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "id": "gsFKRxUnC2OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterating over each sentence in the 'sentences' list\n",
        "for i in range(len(sentences)):\n",
        "    # Removing non-alphabetical characters and replacing them with spaces\n",
        "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
        "\n",
        "    # Converting all characters to lowercase\n",
        "    review = review.lower()\n",
        "\n",
        "    # Splitting the sentence into individual words\n",
        "    review = review.split()\n",
        "\n",
        "    # Lemmatizing each word (reducing them to their base form) if it's not a stopword\n",
        "    review = [lemmatizer.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "\n",
        "    # Joining the lemmatized words back into a sentence\n",
        "    review = ' '.join(review)\n",
        "\n",
        "    # Adding the processed sentence to the 'corpus' list\n",
        "    corpus.append(review)\n"
      ],
      "metadata": {
        "id": "dEF2z3XaFCSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "id": "a9T5JeceGoby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the Bag of Words model Convert text Into Vectre From (0,1)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize CountVectorizer with a maximum of 1500 features (most frequent words)\n",
        "cv = CountVectorizer(max_features=1500)\n",
        "\n",
        "# Fit and transform the 'corpus' data into a sparse matrix of token counts\n",
        "X = cv.fit_transform(corpus).toarray()\n"
      ],
      "metadata": {
        "id": "gb0ixU_ZGP3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "id": "HfuN3WzsGeUT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}