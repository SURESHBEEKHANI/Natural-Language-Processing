{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOkA6vdaF55KNBXEWfggI67",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Natural-Language-Processing/blob/main/faker_news_classifier_using_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fake news classification"
      ],
      "metadata": {
        "id": "-QEAG-TbOr-H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHb9FlVAEFvy"
      },
      "outputs": [],
      "source": [
        "# Importing the Pandas library as 'pd' to work with datasets\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from a CSV file\n",
        "data = pd.read_csv('/content/train.csv')"
      ],
      "metadata": {
        "id": "1hfVCl4sOU-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the first 5 rows of the dataset\n",
        "data.head(5)"
      ],
      "metadata": {
        "id": "cYtYU68fOf1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display information about the dataset\n",
        "data.info()"
      ],
      "metadata": {
        "id": "FBYM-dw6Pdpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the dimensions of the dataset (rows, columns)\n",
        "data.shape\n"
      ],
      "metadata": {
        "id": "ktb5z1NqPZyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for null values in the dataset\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "ojsCy9kPTNc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows with null values from the dataset\n",
        "data = data.dropna()"
      ],
      "metadata": {
        "id": "09Z0PZIhTnKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for null values in the dataset\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "dZxW4IOpT392"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the dimensions of the dataset (rows, columns)\n",
        "data.shape\n"
      ],
      "metadata": {
        "id": "RSd2qFENTx0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the independent features for training by dropping the target variable 'label'\n",
        "X = data.drop('label', axis=1)"
      ],
      "metadata": {
        "id": "jv7EXwdz8Mi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the dependent feature (target variable) for training\n",
        "y = data['label']"
      ],
      "metadata": {
        "id": "jnaYGtrj8z6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the shape of the X and y variables\n",
        "print(\"The shape of X variable:\", X.shape)\n",
        "print(\"The shape of y variable:\", y.shape)"
      ],
      "metadata": {
        "id": "jjPgtbU4-Epu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TensorFlow library\n",
        "import tensorflow as tf\n",
        "\n",
        "# Check the version of TensorFlow\n",
        "tf.__version__"
      ],
      "metadata": {
        "id": "YI6zkCii-Tm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Embedding layer from tensorflow.keras\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "# Import the pad_sequences function for setting sequence representation to one size\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Import the Sequential model from tensorflow.keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Import the one_hot function for one-hot encoding of text\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "\n",
        "# Import the LSTM layer from tensorflow.keras\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "# Import the Dense layer from tensorflow.keras\n",
        "from tensorflow.keras.layers import Dense\n"
      ],
      "metadata": {
        "id": "3Tu_kADh_GrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Provide the Vocabulary Size Of from data\n",
        "vocab_size = 5000"
      ],
      "metadata": {
        "id": "g38HlPqaGChU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One_hot_Representation"
      ],
      "metadata": {
        "id": "aI2F0eKqG0o-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create The Copy  of X Variable  input Vrible\n",
        "messenge = X.copy()"
      ],
      "metadata": {
        "id": "Tm7D0MnLG0Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the index of the DataFrame\n",
        "messenge.reset_index(inplace=True)\n"
      ],
      "metadata": {
        "id": "MOsmXLeYHxBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the nltk library for text preprocessing\n",
        "import nltk\n",
        "\n",
        "# Import the re library for removing punctuations using regular expressions\n",
        "import re\n",
        "\n",
        "# Import the stopwords from nltk.corpus to filter out unmeaningful words\n",
        "from nltk.corpus import stopwords\n"
      ],
      "metadata": {
        "id": "qhYtLPJAH9iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download stopwords from nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "id": "1GdxI8lJI2KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "# Import the PorterStemmer from nltk for reducing word size to their root form\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# Define the PorterStemmer object\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# Define an empty list to store the processed documents\n",
        "corpus = []\n",
        "\n",
        "for i in range(0, len(messenge)):\n",
        "    # Remove punctuation from the title\n",
        "    review = re.sub(r'[^\\w\\s]', '', messenge['title'].iloc[i])\n",
        "    print(i)\n",
        "    # Convert to lowercase\n",
        "    review = review.lower()\n",
        "    # Split the review into words\n",
        "    review = review.split()\n",
        "    # Remove stopwords and apply stemming\n",
        "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
        "    # Join the processed words back into a single string and add to corpus\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)\n"
      ],
      "metadata": {
        "id": "kO7OzQtwK-Ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "id": "FtNcKNJlOx69"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}