{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Natural-Language-Processing/blob/main/faker_news_classifier_using_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fake news classification"
      ],
      "metadata": {
        "id": "-QEAG-TbOr-H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHb9FlVAEFvy"
      },
      "outputs": [],
      "source": [
        "# Importing the Pandas library as 'pd' to work with datasets\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from a CSV file\n",
        "data = pd.read_csv('/content/train.csv')"
      ],
      "metadata": {
        "id": "1hfVCl4sOU-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the first 5 rows of the dataset\n",
        "data.head(5)"
      ],
      "metadata": {
        "id": "cYtYU68fOf1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display information about the dataset\n",
        "data.info()"
      ],
      "metadata": {
        "id": "FBYM-dw6Pdpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the dimensions of the dataset (rows, columns)\n",
        "data.shape\n"
      ],
      "metadata": {
        "id": "ktb5z1NqPZyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for null values in the dataset\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "ojsCy9kPTNc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows with null values from the dataset\n",
        "data = data.dropna()"
      ],
      "metadata": {
        "id": "09Z0PZIhTnKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for null values in the dataset\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "dZxW4IOpT392"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the dimensions of the dataset (rows, columns)\n",
        "data.shape\n"
      ],
      "metadata": {
        "id": "RSd2qFENTx0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the independent features for training by dropping the target variable 'label'\n",
        "X = data.drop('label', axis=1)"
      ],
      "metadata": {
        "id": "jv7EXwdz8Mi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the dependent feature (target variable) for training\n",
        "y = data['label']"
      ],
      "metadata": {
        "id": "jnaYGtrj8z6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the shape of the X and y variables\n",
        "print(\"The shape of X variable:\", X.shape)\n",
        "print(\"The shape of y variable:\", y.shape)"
      ],
      "metadata": {
        "id": "jjPgtbU4-Epu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TensorFlow library\n",
        "import tensorflow as tf\n",
        "\n",
        "# Check the version of TensorFlow\n",
        "tf.__version__"
      ],
      "metadata": {
        "id": "YI6zkCii-Tm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Embedding layer from tensorflow.keras\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "# Import the pad_sequences function for setting sequence representation to one size\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Import the Sequential model from tensorflow.keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Import the one_hot function for one-hot encoding of text\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "\n",
        "# Import the LSTM layer from tensorflow.keras\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "# Import the Dense layer from tensorflow.keras\n",
        "from tensorflow.keras.layers import Dense\n"
      ],
      "metadata": {
        "id": "3Tu_kADh_GrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Provide the Vocabulary Size Of from data\n",
        "vocab_size = 5000"
      ],
      "metadata": {
        "id": "g38HlPqaGChU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text_Preprocessing"
      ],
      "metadata": {
        "id": "aI2F0eKqG0o-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create The Copy  of X Variable  input Vrible\n",
        "messenge = X.copy()"
      ],
      "metadata": {
        "id": "Tm7D0MnLG0Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the index of the DataFrame\n",
        "messenge.reset_index(inplace=True)\n"
      ],
      "metadata": {
        "id": "MOsmXLeYHxBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the nltk library for text preprocessing\n",
        "import nltk\n",
        "\n",
        "# Import the re library for removing punctuations using regular expressions\n",
        "import re\n",
        "\n",
        "# Import the stopwords from nltk.corpus to filter out unmeaningful words\n",
        "from nltk.corpus import stopwords\n"
      ],
      "metadata": {
        "id": "qhYtLPJAH9iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download stopwords from nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "id": "1GdxI8lJI2KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "# Import the PorterStemmer from nltk for reducing word size to their root form\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# Define the PorterStemmer object\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# Define an empty list to store the processed documents\n",
        "corpus = []\n",
        "\n",
        "for i in range(0, len(messenge)):\n",
        "    # Remove punctuation from the title\n",
        "    review = re.sub(r'[^\\w\\s]', '', messenge['title'].iloc[i])\n",
        "    print(i)\n",
        "    # Convert to lowercase\n",
        "    review = review.lower()\n",
        "    # Split the review into words\n",
        "    review = review.split()\n",
        "    # Remove stopwords and apply stemming\n",
        "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
        "    # Join the processed words back into a single string and add to corpus\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)\n"
      ],
      "metadata": {
        "id": "kO7OzQtwK-Ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "id": "dxM_RmHVSepR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One_hot_Representation"
      ],
      "metadata": {
        "id": "RQz_zYM-Wp2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use One_Hot Representation  Functin Convert Text into vecter\n",
        "one_hot_rep = [one_hot(words, vocab_size) for words in corpus]\n",
        "one_hot_rep"
      ],
      "metadata": {
        "id": "sqaNenzqWo14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings Representation"
      ],
      "metadata": {
        "id": "5KkTYuwsaoXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the length of each sentence to 20 words\n",
        "sent_length = 20\n",
        "\n",
        "# Use one-hot encoding to convert the text into numbers, then pad the sequences\n",
        "# 'one_hot_rep' should be the one-hot encoded representation of the text\n",
        "embedded_docs = pad_sequences(one_hot_rep, padding='pre', maxlen=sent_length)\n",
        "\n",
        "# Print the padded and encoded sentences\n",
        "print(embedded_docs)\n"
      ],
      "metadata": {
        "id": "Tpfzm6Yzad90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The Length Of Embedded_docs\n",
        "print(len(embedded_docs))"
      ],
      "metadata": {
        "id": "IzvusqPRb02j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of sentences in the padded and encoded data\n",
        "print(len(embedded_docs))\n"
      ],
      "metadata": {
        "id": "x2mHvoUhcIk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print the Embedding Doucments Throug Index\n",
        "print(embedded_docs[100])"
      ],
      "metadata": {
        "id": "jQ4peNFVc3E7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define The Dimmminsion Of Model\n",
        "dim=40\n",
        "# Create the LSTM model\n",
        "model = Sequential()\n",
        "\n",
        "# Add an embedding layer\n",
        "# input_dim is the size of the vocabulary\n",
        "# output_dim is the dimension of the dense embedding\n",
        "# input_length is the length of input sequences\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=40, input_length=sent_length))\n",
        "\n",
        "# Add an LSTM layer with 100 units\n",
        "model.add(LSTM(100))\n",
        "\n",
        "# Add a Dense layer with 1 unit and a sigmoid activation function for binary classification\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "#Compile The Model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "1IYzCUOud6pY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Summary  of model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "4CLYWI4lef-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Converting embedded_docs to a numpy array\n",
        "X_final = np.array(embedded_docs)\n",
        "\n",
        "# Converting y to a numpy array\n",
        "y_final = np.array(y)\n"
      ],
      "metadata": {
        "id": "tOv3VTngfp0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split the dataset For Test and Training"
      ],
      "metadata": {
        "id": "wtKP3rxCgMKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# 30% of the data will be used for testing, and the rest for training\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_final, y_final, test_size=0.30, random_state=42)\n",
        "\n",
        "# Print the training and testing data\n",
        "print(\"X_train:\", X_train)\n",
        "print(\"X_test:\", X_test)\n",
        "print(\"Y_train:\", Y_train)\n",
        "print(\"Y_test:\", Y_test)\n"
      ],
      "metadata": {
        "id": "K6eY5vR-g0n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model_Training"
      ],
      "metadata": {
        "id": "MZ4kohcoiDAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with training data and validate using testing data\n",
        "# 'epochs' specifies the number of times the model will go through the entire training dataset\n",
        "# 'batch_size' specifies the number of samples per gradient update\n",
        "\n",
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=5, batch_size=64)\n"
      ],
      "metadata": {
        "id": "az24eFwLiCfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Dropout"
      ],
      "metadata": {
        "id": "epsaJrZA8f2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "embedding_vector_features = 40\n",
        "# Initialize the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add embedding layer\n",
        "# 'vocab_size' is the size of the vocabulary\n",
        "# 'embedding_vector_features' is the dimension of the dense embedding\n",
        "# 'input_length' is the length of input sequences\n",
        "model.add(Embedding(vocab_size, embedding_vector_features, input_length=sent_length))\n",
        "\n",
        "# Add dropout layer to prevent overfitting\n",
        "# '0.3' is the dropout rate, which means 30% of the neurons will be randomly set to zero\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Add LSTM layer\n",
        "# '100' is the number of units in the LSTM layer\n",
        "model.add(LSTM(100))\n",
        "\n",
        "# Add another dropout layer to prevent overfitting\n",
        "# '0.3' is the dropout rate\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Add output layer\n",
        "# 'Dense(1)' means we have one output neuron\n",
        "# 'activation='sigmoid'' means we use the sigmoid activation function for binary classification\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "# 'loss='binary_crossentropy'' is the loss function used for binary classification\n",
        "# 'optimizer='adam'' is the optimizer used to update the weights\n",
        "# 'metrics=['accuracy']' means we want to track accuracy during training\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "R2y2082X8fTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance Metrics And Accuracy"
      ],
      "metadata": {
        "id": "xCO-yNpb9xHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Predict classes using model.predict and np.argmax\n",
        "y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(Y_test, y_pred)\n",
        "\n",
        "# Compute accuracy score\n",
        "accuracy = accuracy_score(Y_test, y_pred)\n",
        "\n",
        "# Print the confusion matrix and accuracy score\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "8c68dQ1O2bSv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}