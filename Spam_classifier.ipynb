{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMH6FMDw1X8ujOlbqABRAIj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Natural-Language-Processing/blob/main/Spam_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DCedmsopEps"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "message = pd.read_csv('/content/SMSSpamCollection.txt', sep='\\t', header=None)\n",
        "message.columns = ['label', 'message']\n",
        "message.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Text data cleaning  and Preprocessing  use library  Import\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer"
      ],
      "metadata": {
        "id": "IVzTLbjsuZwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the re library for regular expressions\n",
        "import re\n",
        "# Importing the nltk library for natural language processing tasks\n",
        "import nltk\n",
        "\n",
        "# Downloading the stopwords from the nltk library\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Importing the stopwords from the nltk library use for the unnecessary and Without Meangfull word Remved\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Importing the PorterStemmer from the nltk library to perform stemming in the daataset and Provide Reduce  data set\n",
        "from nltk.stem.porter import PorterStemmer\n"
      ],
      "metadata": {
        "id": "LLt1FGJpAyoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the PorterStemmer\n",
        "stemming = PorterStemmer()\n",
        "\n",
        "# Create an empty list to store the cleaned messages (corpus)\n",
        "corpus = []\n",
        "\n",
        "# Loop through each message in the dataset\n",
        "for i in range(0, len(message)):\n",
        "    # Step 1: Remove non-alphabetic characters from the message\n",
        "    # re.sub('[^a-zA-Z]', ' ', message['message'][i]) replaces anything that is not a letter with a space\n",
        "    review = re.sub('[^a-zA-Z]', ' ', message['message'][i])\n",
        "\n",
        "    # Step 2: Convert the text to lowercase to maintain uniformity\n",
        "    review = review.lower()\n",
        "\n",
        "    # Step 3: Split the text into individual words\n",
        "    review = review.split()\n",
        "\n",
        "    # Step 4: Remove stopwords and perform stemming on each word in the review\n",
        "    # [stemming.stem(word) for word in review if not word in stopwords.words('english')]\n",
        "    # keeps only the words that are not stopwords and stems them\n",
        "    review = [stemming.stem(word) for word in review if not word in stopwords.words('english')]\n",
        "\n",
        "    # Step 5: Join the cleaned words back into a single string\n",
        "    review = ' '.join(review)\n",
        "\n",
        "    # Step 6: Add the cleaned review to the corpus\n",
        "    corpus.append(review)\n"
      ],
      "metadata": {
        "id": "t47_vWJ5FOj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "id": "JxKUlci3GSlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use Bag of words Convert Text into Numerical value\n",
        "# Import the CountVectorizer from the sklearn library\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize the CountVectorizer with a maximum of 2500 features\n",
        "# CountVectorizer converts text into numerical values by counting word occurrences\n",
        "cv = CountVectorizer(max_features=5000)\n",
        "\n",
        "# Fit the CountVectorizer to the corpus and transform the text data into a numerical array\n",
        "# The fit_transform method learns the vocabulary and converts text into a document-term matrix\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "\n",
        "# X now contains the numerical representation of the text data\n",
        "# Each row in X corresponds to a document (text) in the corpus\n",
        "# Each column corresponds to a word in the vocabulary (up to 2500 words)\n",
        "# The values in X are the counts of each word in each document\n",
        "\n",
        "# Assuming 'message' is a DataFrame and 'label' is a column containing categorical labels\n",
        "\n",
        "# pd.get_dummies(message['label']) creates dummy variables for each unique value in the 'label' column\n",
        "y = pd.get_dummies(message['label'])\n",
        "\n",
        "# Select the 'spam' column from the dummy variables and assign it to 'y'\n",
        "y = y.iloc[:, 1].values\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the data\n",
        "Y= label_encoder.fit_transform(y)"
      ],
      "metadata": {
        "id": "3TXNuBz3GuLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "id": "y1eF27lklBbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary function from sklearn library to split data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "# X represents the features (input data), y represents the target variable (output data)\n",
        "# test_size=0.20 specifies that 20% of the data will be used for testing, and 80% for training\n",
        "# random_state=0 ensures that the split is reproducible, i.e., the same split occurs each time the code is run\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=0)\n"
      ],
      "metadata": {
        "id": "kS--kpPuhHu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Multinomial Naive Bayes class from the scikit-learn library\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "# Import the accuracy_score function from the scikit-learn library\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Import the pickle module for saving the model\n",
        "import pickle\n",
        "\n",
        "# Train the Naive Bayes model with the training data (X_train) and their labels (Y_train)\n",
        "spam_detect_model = MultinomialNB().fit(X_train, Y_train)\n",
        "\n",
        "# Use the trained model to make predictions on the test data (X_test)\n",
        "Y_pred = spam_detect_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(Y_test, Y_pred)\n",
        "\n",
        "# Print the accuracy of the model\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Save the trained model to a file using pickle\n",
        "with open('spam_detect_model.pkl', 'wb') as model_file:\n",
        "    pickle.dump(spam_detect_model, model_file)\n",
        "\n",
        "# Save the vectorizer used to transform the data if needed (assuming you have a vectorizer)\n",
        "# with open('vectorizer.pkl', 'wb') as vectorizer_file:\n",
        "#     pickle.dump(vectorizer, vectorizer_file)\n",
        "\n"
      ],
      "metadata": {
        "id": "VIpd7G4DlOVD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}