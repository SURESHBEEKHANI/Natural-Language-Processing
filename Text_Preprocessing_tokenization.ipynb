{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMM9C7MGGC6qqknMS39QCvW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Natural-language-processing-/blob/main/Text_Preprocessing_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "\n",
        "Tokenization is the process of converting text into smaller pieces, called tokens. Tokens can be words, phrases, or even characters. The goal of tokenization is to break down text into meaningful units that can be used for further processing in various natural language processing (NLP) tasks, such as text analysis, machine learning, and information retrieval.\n",
        "\n",
        "There are different types of tokenization methods, depending on the language and the specific requirements of the task:\n",
        "\n",
        "#Word Tokenization:\n",
        "\n",
        "Splitting text into individual words. For example:\n",
        "\n",
        "Input: \"The quick brown fox jumps over the lazy dog.\"\n",
        "Output: [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
        "\n",
        "#Sentence Tokenization:\n",
        "\n",
        "Splitting text into sentences. For example:\n",
        "\n",
        "Input: \"Hello world. How are you?\"\n",
        "Output: [\"Hello world.\", \"How are you?\"]\n",
        "\n",
        "#Character Tokenization:\n",
        "Splitting text into individual characters. For example:\n",
        "\n",
        "Input: \"Hello\"\n",
        "Output: [\"H\", \"e\", \"l\", \"l\", \"o\"]\n",
        "\n",
        "#Subword Tokenization:\n",
        "\n",
        "Splitting text into subwords or morphemes. This is particularly useful for handling out-of-vocabulary words in languages with rich morphology. For example:\n",
        "\n",
        "Input: \"unhappiness\"\n",
        "Output: [\"un\", \"happiness\"]\n",
        "\n",
        "#N-gram Tokenization:\n",
        "\n",
        "Splitting text into contiguous sequences of n items. For example, with n=2 (bigrams):\n",
        "\n",
        "Input: \"The quick brown fox\"\n",
        "Output: [\"The quick\", \"quick brown\", \"brown fox\"]\n",
        "\n",
        "Tokenization can be more complex for languages that do not use spaces to separate words (e.g., Chinese, Japanese). In such cases, more advanced techniques such as dictionary-based tokenization, statistical models, or machine learning approaches are used.\n",
        "\n",
        "Tokenization is a crucial preprocessing step in many NLP applications, as it converts raw text into a structured format that can be easily analyzed and processed by algorithms.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VqQvWLh9KkwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "BgaE17b6afRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iJKVqY4Jak0"
      },
      "outputs": [],
      "source": [
        "!pip  install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paragraph to be converted into sentences Use Sent_tokenizer"
      ],
      "metadata": {
        "id": "M19jlE8BT7nx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the sent_tokenize function from the nltk.tokenize module\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Paragraph to be converted into sentences\n",
        "paragraph = \"\"\"Artificial Intelligence (AI) is transforming the world.\n",
        "It is being used in various fields such as healthcare, finance, and transportation.\n",
        "Many experts believe that AI will continue to evolve and become an integral part of our daily lives.\"\"\"\n",
        "\n",
        "# Use the sent_tokenize function to split the paragraph into sentences\n",
        "sentences = sent_tokenize(paragraph)\n",
        "\n",
        "# Print each sentence on a new line\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bd81Av18TIK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paragraph or sentence to be converted into Word Use word_tokenizer"
      ],
      "metadata": {
        "id": "QrCZn-ZDVVPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the word_tokenize function from the nltk.tokenize module\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Paragraph to be converted into words\n",
        "paragraph = \"\"\"Artificial Intelligence (AI) is transforming the world.\n",
        "It is being used in various fields such as healthcare, finance, and transportation.\n",
        "Many experts believe that AI will continue to evolve and become an integral part of our daily lives.\"\"\"\n",
        "\n",
        "# Use the word_tokenize function to split the paragraph into words\n",
        "words = word_tokenize(paragraph)\n",
        "\n",
        "# Print each word on a new line\n",
        "for word in words:\n",
        "    print(word)\n"
      ],
      "metadata": {
        "id": "KEZ8ol_XT2SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the wordpunct_tokenize function from the nltk.tokenize module\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "\n",
        "# Use wordpunct_tokenize to split the paragraph into individual words and punctuation marks\n",
        "words = wordpunct_tokenize(paragraph)\n",
        "\n",
        "# Loop through each word (or punctuation mark) in the list of words\n",
        "for word in words:\n",
        "    # Print each word (or punctuation mark)\n",
        "    print(word)\n"
      ],
      "metadata": {
        "id": "ytDZT3WTYSm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the TreebankWordTokenizer class from the nltk.tokenize module\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "\n",
        "# Create an instance of the TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "\n",
        "# Use the tokenizer to split the paragraph into individual words and punctuation marks\n",
        "words = tokenizer.tokenize(paragraph)\n",
        "\n",
        "# Loop through each word (or punctuation mark) in the list of words\n",
        "for word in words:\n",
        "    # Print each word (or punctuation mark)\n",
        "    print(word)\n"
      ],
      "metadata": {
        "id": "iOKAvvx1gAuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Paragraph or sentence to be converted into Character  Use word_tokenizer"
      ],
      "metadata": {
        "id": "l2TKm9GgkBBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paragraph to be converted into words\n",
        "paragraph = \"\"\"Artificial Intelligence (AI) is transforming the world.\n",
        "It is being used in various fields such as healthcare, finance, and transportation.\n",
        "Many experts believe that AI will continue to evolve and become an integral part of our daily lives.\"\"\"\n",
        "# Tokenize the paragraph into characters\n",
        "characters = list(paragraph)\n",
        "\n",
        "# Print each character token\n",
        "for char in characters:\n",
        "    print(char)\n"
      ],
      "metadata": {
        "id": "TRKYEB1hi2Ou"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}