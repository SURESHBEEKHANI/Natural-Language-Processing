{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMrC+IyZl2DxwKiBMyn/bY2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Natural-language-processing-/blob/main/Text_Preprocessing_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "\n",
        "Tokenization is the process of converting text into smaller pieces, called tokens. Tokens can be words, phrases, or even characters. The goal of tokenization is to break down text into meaningful units that can be used for further processing in various natural language processing (NLP) tasks, such as text analysis, machine learning, and information retrieval.\n",
        "\n",
        "There are different types of tokenization methods, depending on the language and the specific requirements of the task:\n",
        "\n",
        "#Word Tokenization:\n",
        "\n",
        "Splitting text into individual words. For example:\n",
        "\n",
        "Input: \"The quick brown fox jumps over the lazy dog.\"\n",
        "Output: [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
        "\n",
        "#Sentence Tokenization:\n",
        "\n",
        "Splitting text into sentences. For example:\n",
        "\n",
        "Input: \"Hello world. How are you?\"\n",
        "Output: [\"Hello world.\", \"How are you?\"]\n",
        "\n",
        "#Character Tokenization:\n",
        "Splitting text into individual characters. For example:\n",
        "\n",
        "Input: \"Hello\"\n",
        "Output: [\"H\", \"e\", \"l\", \"l\", \"o\"]\n",
        "\n",
        "#Subword Tokenization:\n",
        "\n",
        "Splitting text into subwords or morphemes. This is particularly useful for handling out-of-vocabulary words in languages with rich morphology. For example:\n",
        "\n",
        "Input: \"unhappiness\"\n",
        "Output: [\"un\", \"happiness\"]\n",
        "\n",
        "#N-gram Tokenization:\n",
        "\n",
        "Splitting text into contiguous sequences of n items. For example, with n=2 (bigrams):\n",
        "\n",
        "Input: \"The quick brown fox\"\n",
        "Output: [\"The quick\", \"quick brown\", \"brown fox\"]\n",
        "\n",
        "Tokenization can be more complex for languages that do not use spaces to separate words (e.g., Chinese, Japanese). In such cases, more advanced techniques such as dictionary-based tokenization, statistical models, or machine learning approaches are used.\n",
        "\n",
        "Tokenization is a crucial preprocessing step in many NLP applications, as it converts raw text into a structured format that can be easily analyzed and processed by algorithms.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VqQvWLh9KkwO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iJKVqY4Jak0"
      },
      "outputs": [],
      "source": [
        "!pip  install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paragraph to be converted into sentences Use Sent_tokenizer"
      ],
      "metadata": {
        "id": "M19jlE8BT7nx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the sent_tokenize function from the nltk.tokenize module\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Paragraph to be converted into sentences\n",
        "paragraph = \"\"\"Artificial Intelligence (AI) is transforming the world.\n",
        "It is being used in various fields such as healthcare, finance, and transportation.\n",
        "Many experts believe that AI will continue to evolve and become an integral part of our daily lives.\"\"\"\n",
        "\n",
        "# Use the sent_tokenize function to split the paragraph into sentences\n",
        "sentences = sent_tokenize(paragraph)\n",
        "\n",
        "# Print each sentence on a new line\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n"
      ],
      "metadata": {
        "id": "Bd81Av18TIK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paragraph or sentence to be converted into Word Use word_tokenizer"
      ],
      "metadata": {
        "id": "QrCZn-ZDVVPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the word_tokenize function from the nltk.tokenize module\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Paragraph to be converted into words\n",
        "paragraph = \"\"\"Artificial Intelligence (AI) is transforming the world.\n",
        "It is being used in various fields such as healthcare, finance, and transportation.\n",
        "Many experts believe that AI will continue to evolve and become an integral part of our daily lives.\"\"\"\n",
        "\n",
        "# Use the word_tokenize function to split the paragraph into words\n",
        "words = word_tokenize(paragraph)\n",
        "\n",
        "# Print each word on a new line\n",
        "for word in words:\n",
        "    print(word)\n"
      ],
      "metadata": {
        "id": "KEZ8ol_XT2SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ytDZT3WTYSm-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}