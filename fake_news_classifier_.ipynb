{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOmQt2RWEw8JLUMHvQZS1n5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Natural-Language-Processing/blob/main/fake_news_classifier_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx0FHXN0ynwL"
      },
      "outputs": [],
      "source": [
        "# Importing pandas library and aliasing it as pd\n",
        "import pandas as pd\n",
        "\n",
        "# Importing text preprocessing techniques from the sklearn library\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
        "\n",
        "# Importing stopwords from the nltk.corpus module\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Importing PorterStemmer from the nltk.stem module\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Importing re module for regular expressions\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the CSV file into a DataFram\n",
        "df = pd.read_csv('/content/train.csv')"
      ],
      "metadata": {
        "id": "VWeTC0598B9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the first few rows of the DataFrame\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ctZ_Xw8kCyDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the target variable column named 'label' from the dataset\n",
        "x = df.drop('label', axis=1)\n"
      ],
      "metadata": {
        "id": "O-kox3vND01f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the first few rows of the DataFrame x\n",
        "x.head()\n"
      ],
      "metadata": {
        "id": "8oDiabuoEHU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning the 'label' column from DataFrame df to variable y\n",
        "y = df['label']\n"
      ],
      "metadata": {
        "id": "Iv7K1TMKFHsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the first few values of the Series y (from the 'label' column)\n",
        "y.head()\n"
      ],
      "metadata": {
        "id": "wBDNj5EPFL1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieving the dimensions (shape) of the DataFrame df\n",
        "df.shape\n"
      ],
      "metadata": {
        "id": "9UA4ZrwIFqJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing rows with null values from the DataFrame df\n",
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "ClXt54I3Gqhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieving the dimensions (shape) of the DataFrame df\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Dp9EuUmvHAmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the first 10 rows of the DataFrame df\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "uSyRp8dSHIWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy of the dataset DataFrame df\n",
        "messages = df.copy()\n"
      ],
      "metadata": {
        "id": "xL_LZN2cH1OJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resetting the index of the dataset DataFrame messages to be sequential and ordered\n",
        "messages.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "0PBOPcK_ICs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages.head(10)"
      ],
      "metadata": {
        "id": "0cE2Orb_J6oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the first 10 rows of the DataFrame messages\n",
        "messages.head(10)\n"
      ],
      "metadata": {
        "id": "Wvgg463aK53_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the stopwords dataset from NLTK\n",
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "id": "lrVzGxunM3nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing a PorterStemmer instance\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# Initializing an empty list to store processed text\n",
        "corpus = []\n",
        "\n",
        "# Iterating through each row in the 'title' column of the DataFrame messages\n",
        "for i in range(0, len(messages)):\n",
        "    # Removing non-alphabetic characters and replacing them with a space\n",
        "    review = re.sub('[^a-zA-Z]', ' ', messages['title'][i])\n",
        "    # Converting all characters to lowercase\n",
        "    review = review.lower()\n",
        "    # Splitting the text into a list of words\n",
        "    review = review.split()\n",
        "    # Applying stemming using PorterStemmer and excluding stopwords\n",
        "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
        "    # Joining the stemmed words back into a single string separated by spaces\n",
        "    review = ' '.join(review)\n",
        "    # Appending the processed text to the corpus list\n",
        "    corpus.append(review)\n"
      ],
      "metadata": {
        "id": "8NIALv0RNfVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "id": "NVWHcvlfdcIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply CountVectorizer to create a Bag of Words model\n",
        "# Convert text into vectors\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize CountVectorizer with parameters:\n",
        "# - max_features=5000: Limits the number of features (words or n-grams) to the top 5000 by frequency.\n",
        "# - ngram_range=(1,3): Considers unigrams, bigrams, and trigrams as features.\n",
        "cv = CountVectorizer(max_features=5000, ngram_range=(1, 3))\n",
        "\n",
        "# Transform the corpus (list of preprocessed text) into a matrix of token counts (X)\n",
        "X = cv.fit_transform(corpus).toarray()\n"
      ],
      "metadata": {
        "id": "YXYDdoN6eY5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "id": "uYom3XWDfmdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieving the dimensions (shape) of the matrix X\n",
        "X.shape\n",
        "\n"
      ],
      "metadata": {
        "id": "l0M6yyoxekln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning the 'label' column from the messages DataFrame to Y\n",
        "Y = messages['label']"
      ],
      "metadata": {
        "id": "t68E7nsKx55C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Using train_test_split to divide the data into:\n",
        "# - X_train: Training data for features (X)\n",
        "# - X_test: Testing data for features (X)\n",
        "# - Y_train: Training data for target variable (Y)\n",
        "# - Y_test: Testing data for target variable (Y)\n",
        "# test_size=0.33 indicates that 33% of the data will be used for testing, and 67% for training\n",
        "# random_state=0 ensures reproducibility of results\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=0)\n"
      ],
      "metadata": {
        "id": "Za3u5mouxWT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming X_train, X_test, Y_train, Y_test are already defined\n",
        "# Example shapes:\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of Y_train:\", Y_train.shape)\n",
        "print(\"Shape of Y_test:\", Y_test.shape)"
      ],
      "metadata": {
        "id": "7RcqT3S6fGcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'cv' is your CountVectorizer object\n",
        "# Retrieve the feature names from the CountVectorizer object\n",
        "feature_names = cv.get_feature_names_out()\n",
        "\n",
        "# Print the first 20 feature names\n",
        "print(feature_names[:20])\n",
        "\n"
      ],
      "metadata": {
        "id": "Zx8gP6jxfcsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the parameters of the CountVectorizer object\n",
        "cv.get_params()\n"
      ],
      "metadata": {
        "id": "ZC3LNT8XhKKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the feature names using get_feature_names_out\n",
        "feature_names = cv.get_feature_names_out()\n",
        "\n",
        "# Create a DataFrame using the feature names as column headers\n",
        "count_df = pd.DataFrame(X_train,columns=feature_names)\n",
        "# Print the first few rows of the DataFrame\n",
        "print(count_df.head())"
      ],
      "metadata": {
        "id": "EQzC_9hChS-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    See full source and example:\n",
        "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "\n",
        "    Parameters:\n",
        "    - cm: Confusion matrix, a 2D numpy array.\n",
        "    - classes: List of class labels (e.g., ['class1', 'class2']).\n",
        "    - normalize: If True, normalize the confusion matrix.\n",
        "    - title: Title of the plot.\n",
        "    - cmap: Color map for the plot.\n",
        "    \"\"\"\n",
        "    # Plotting the confusion matrix as an image plot\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)  # Setting the title of the plot\n",
        "    plt.colorbar()    # Adding a color bar to the plot\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)  # Setting x-axis labels with rotation\n",
        "    plt.yticks(tick_marks, classes)              # Setting y-axis labels\n",
        "\n",
        "    # Normalizing the confusion matrix if normalize=True\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    # Setting the threshold for text color based on the maximum value in the confusion matrix\n",
        "    thresh = cm.max() / 2.\n",
        "\n",
        "    # Adding text annotations to each cell of the plot\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], '.2f' if normalize else 'd'),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    # Adjusting layout and setting labels for y-axis and x-axis\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n"
      ],
      "metadata": {
        "id": "Y7xh1NAbkThV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the Multinomial Naive Bayes classifier from scikit-learn\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Creating an instance of the Multinomial Naive Bayes classifier\n",
        "classifier = MultinomialNB()\n"
      ],
      "metadata": {
        "id": "qAHcptKUk37U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary modules for metrics and utility functions\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import itertools\n"
      ],
      "metadata": {
        "id": "9BylcXhKlk-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the classifier on the training data\n",
        "classifier.fit(X_train, Y_train)\n",
        "\n",
        "# Making predictions on the test data\n",
        "pred = classifier.predict(X_test)\n",
        "\n",
        "# Calculating the accuracy score of the classifier\n",
        "score = metrics.accuracy_score(Y_test, pred)\n",
        "print(\"Accuracy: %0.3f\" % score)\n",
        "\n",
        "# Generating the confusion matrix\n",
        "cm = metrics.confusion_matrix(Y_test, pred)\n",
        "\n",
        "# Plotting the confusion matrix\n",
        "plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])\n"
      ],
      "metadata": {
        "id": "6oJMkqjtlFwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the classifier on the training data\n",
        "classifier.fit(X_train, Y_train)\n",
        "\n",
        "# Making predictions on the test data\n",
        "pred = classifier.predict(X_test)\n",
        "\n",
        "# Calculating the accuracy score of the classifier\n",
        "score = metrics.accuracy_score(Y_test, pred)\n",
        "\n",
        "# Outputting the accuracy score\n",
        "score\n"
      ],
      "metadata": {
        "id": "XSdSIHdNmr6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Passive Aggressive Classifier Algorithm"
      ],
      "metadata": {
        "id": "C78lw5Nenkg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the PassiveAggressiveClassifier from scikit-learn\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "\n",
        "# Initializing the Passive Aggressive Classifier\n",
        "linear_clf = PassiveAggressiveClassifier()\n"
      ],
      "metadata": {
        "id": "J7E4rCxMoFwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qSECDm2TzXed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the classifier\n",
        "linear_clf = PassiveAggressiveClassifier()\n",
        "\n",
        "# Fit the classifier\n",
        "linear_clf.fit(X_train, Y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "pred = linear_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "score = metrics.accuracy_score(Y_test, pred)\n",
        "print(\"accuracy:   %0.3f\" % score)\n",
        "\n",
        "# Compute and print confusion matrix\n",
        "cm = metrics.confusion_matrix(Y_test, pred)\n",
        "print(\"confusion matrix:\")\n",
        "print(cm)\n",
        "plot_confusion_matrix(cm, classes=['FAKE Data', 'REAL Data'])"
      ],
      "metadata": {
        "id": "bEjDfD7foFdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multinomial Classifier with Hyperparameter"
      ],
      "metadata": {
        "id": "BTJM0jP4pt_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a Multinomial Naive Bayes classifier with alpha=0.1\n",
        "classifier = MultinomialNB(alpha=0.1)\n"
      ],
      "metadata": {
        "id": "1jubZJaCnoNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "previous_score = 0\n",
        "\n",
        "# Loop through different values of alpha from 0 to 1 (exclusive), with a step of 0.1\n",
        "for alpha in np.arange(0, 1, 0.1):\n",
        "    # Initialize a Multinomial Naive Bayes classifier with the current alpha value\n",
        "    sub_classifier = MultinomialNB(alpha=alpha)\n",
        "\n",
        "    # Train the classifier using the training data\n",
        "    sub_classifier.fit(X_train, Y_train)\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    y_pred = sub_classifier.predict(X_test)\n",
        "\n",
        "    # Calculate the accuracy score of the predictions\n",
        "    score = metrics.accuracy_score(Y_test, y_pred)\n",
        "\n",
        "    # Check if the current score is higher than the previous highest score\n",
        "    if score > previous_score:\n",
        "        classifier = sub_classifier  # Update the classifier to the current best classifier\n",
        "        previous_score = score  # Update the previous highest score\n",
        "\n",
        "    # Print the alpha value and corresponding score\n",
        "    print(\"Alpha: {}, Score: {}\".format(alpha, score))\n"
      ],
      "metadata": {
        "id": "pQ-4dGKDp2iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide the dataset into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Using train_test_split to split the features (X) and target variable (Y) into training and testing sets\n",
        "# test_size=0.33 indicates that 33% of the data will be used for testing, and 67% for training\n",
        "# random_state=0 ensures reproducibility of results\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=0)\n"
      ],
      "metadata": {
        "id": "9W9BAbsEsxv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the Multinomial Naive Bayes classifier from scikit-learn\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Initializing the Multinomial Naive Bayes classifier\n",
        "classifier = MultinomialNB()\n",
        "\n",
        "# Training the classifier using the training data X_train and labels Y_train\n",
        "classifier.fit(X_train, Y_train)\n",
        "\n",
        "# Making predictions on the test data X_test\n",
        "pred = classifier.predict(X_test)\n",
        "\n",
        "# Calculating the accuracy score of the predictions\n",
        "score = metrics.accuracy_score(Y_test, pred)\n",
        "print(\"Accuracy:   %0.3f\" % score)\n",
        "\n",
        "# Computing the confusion matrix between the true labels Y_test and predicted labels pred\n",
        "cm = metrics.confusion_matrix(Y_test, pred)\n",
        "\n",
        "# Plotting the confusion matrix with specified class labels ['FAKE', 'REAL']\n",
        "plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])\n"
      ],
      "metadata": {
        "id": "pYY7LfPysxoL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}